<div align="center">



<p align="center">
  <img src="docs/imgs/AutoRound.png" alt="AutoRound Overview" width="20%">
</p>


<h3> é¢å‘ LLM çš„é«˜çº§é‡åŒ–ç®—æ³•</h3>

[![python](https://img.shields.io/badge/python-3.10%2B-blue)](https://github.com/intel/auto-round)
[![version](https://img.shields.io/badge/release-0.9.5-green)](https://github.com/intel/auto-round)
[![license](https://img.shields.io/badge/license-Apache%202-9C27B0)](https://github.com/intel/auto-round/blob/main/LICENSE)
<a href="https://huggingface.co/Intel">
<img alt="Model Checkpoints" src="https://img.shields.io/badge/%F0%9F%A4%97%20HF-Models-F57C00">
</a>

[English](README.md) | ç®€ä½“ä¸­æ–‡

---
<div align="left">

## ğŸš€ AutoRound æ˜¯ä»€ä¹ˆï¼Ÿ

AutoRound æ˜¯é¢å‘**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é«˜çº§é‡åŒ–å·¥å…·ã€‚å®ƒé€šè¿‡å¼•å…¥ç¬¦å·æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼ˆsign-gradient descentï¼‰** ï¼Œåªéœ€è¿›è¡Œæå°‘çš„è°ƒå‚ï¼Œå°±èƒ½åœ¨ **æä½ç²¾åº¦ï¼ˆ2â€“4 bitsï¼‰** ä¸‹ä¿æŒè¾ƒé«˜çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¹Ÿå…·å¤‡è¾ƒå¥½çš„ç¡¬ä»¶å…¼å®¹æ€§ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒè®ºæ–‡ [SignRoundV1](https://arxiv.org/pdf/2309.05516) å’Œ [SignRoundV2](http://arxiv.org/abs/2512.04746)ã€‚ä½¿ç”¨è¯´æ˜è¯·å‚é˜… [ç”¨æˆ·æŒ‡å—](./docs/step_by_step.md).

<p align="center">
  <img src="docs/imgs/autoround_overview.png" alt="AutoRound Overview" width="80%">
</p>


## ğŸ†• æœ€æ–°è¿›å±•

* [2025/12] **SignRoundV2** è®ºæ–‡å·²å‘å¸ƒã€‚å¼€å¯ `enable_alg_ext` å¹¶ä½¿ç”¨ **AutoScheme** API è¿›è¡Œæ··åˆç²¾åº¦é‡åŒ–å³å¯å¤ç°è®ºæ–‡å®éªŒç»“æœã€‚è¯¦è§ï¼š[*è®ºæ–‡*](http://arxiv.org/abs/2512.04746)ï¼Œ[*LLaMA æ¨¡å‹è¯„ä¼°è¯´æ˜*](./docs/alg_202508.md)ã€‚

* [2025/11]  **LLM-Compressor** å·²æ”¯æŒAutoRoundç®—æ³•ã€‚è¯¦è§ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://github.com/vllm-project/llm-compressor/tree/main/examples/autoround/README.md)ï¼Œ[*vLLM åšå®¢*](https://blog.vllm.ai/2025/12/09/intel-autoround-llmc.html)ï¼Œ[*RedHat åšå®¢*](https://developers.redhat.com/articles/2025/12/09/advancing-low-bit-quantization-llms-autoround-x-llm-compressor)ï¼Œ[*X æ¨æ–‡*](https://x.com/vllm_project/status/1998710451312771532)ï¼Œ[*Intel åšå®¢*](https://community.intel.com/t5/Blogs/Products-and-Solutions/HPC/Advancing-Low-Bit-Quantization-for-LLMs-AutoRound-x-LLM/post/1729336)ï¼Œ[*LinkedIn*](https://www.linkedin.com/posts/vllm-project_advancing-lowbit-quantization-for-llms-activity-7404478053768441856-ru8f/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAapNW8BLnAdCAr57GOwSCJXjf76ZvOEOAg)ï¼Œ[*å¾®ä¿¡*](https://mp.weixin.qq.com/s/l5WA-1_4ipffQN6GOH2Iqg)ï¼Œ[*çŸ¥ä¹*](https://zhuanlan.zhihu.com/p/1982167638315664412)ã€‚

* [2025/11] æä¾›äº† **å¢å¼ºç‰ˆ GGUF** é‡åŒ–ç®—æ³•ï¼Œå¼€å¯ `--enable_alg_ext`å³å¯ ã€‚[*å‡†ç¡®åº¦*](./docs/gguf_alg_ext_acc.md)æä¾›äº†å°‘é‡å‡†ç¡®ç‡æ•°æ®ã€‚

* [2025/10] AutoRound å·²é›†æˆè‡³ **SGLang**ã€‚è¯¦è§ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://docs.sglang.io/advanced_features/quantization.html#using-auto-round)ï¼Œ[*LMSYS åšå®¢*](https://lmsys.org/blog/2025-11-13-AutoRound/)ï¼Œ[*X æ¨æ–‡*](https://x.com/lmsysorg/status/1991977019220148650?s=20)ï¼Œ[*Intel åšå®¢*](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/AutoRound-Meets-SGLang-Enabling-Quantized-Model-Inference-with/post/1727196)ï¼Œ[*LinkedIn*](https://www.linkedin.com/feed/update/urn:li:activity:7397742859354857472)ã€‚

* [2025/10] æä¾› **æ··åˆç²¾åº¦** ç®—æ³•ï¼Œå¯åœ¨å‡ åˆ†é’Ÿå†…è‡ªåŠ¨ç”Ÿæˆæ··åˆbitæ–¹æ¡ˆã€‚è¯¦è§ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme)ï¼Œ[*å‡†ç¡®åº¦*](./docs/auto_scheme_acc.md)ã€‚

* [2025/09] æ”¯æŒ **MXFP4** å’Œ **NVFP4** æ•°æ®ç±»å‹ã€‚è¯¦è§ï¼š[*å‡†ç¡®åº¦*](./docs/mxnv_acc.md)ã€‚

* [2025/08] ` æä¾› **æ”¹è¿›ç‰ˆ INT2** ç®—æ³•, è¯·å¼€å¯ `--enable_alg_extã€‚è¯¦è§ï¼š[*å‡†ç¡®åº¦*](./docs/alg_202508.md)ã€‚

* [2025/07] æ”¯æŒ **GGUF** æ ¼å¼ã€‚è¯¦è§ï¼š[*ä½¿ç”¨æ–¹æ³•*](./docs/step_by_step.md#gguf-format)ã€‚

* [2025/05] AutoRound å·²é›†æˆè‡³ **vLLM**ã€‚è¯¦è§ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://docs.vllm.ai/en/latest/features/quantization/auto_round/)ï¼Œ[*Medium åšå®¢*](https://medium.com/@NeuralCompressor/accelerating-vllm-and-sglang-deployment-using-autoround-45fdc0b2683e)ï¼Œ[*å°çº¢ä¹¦*](https://www.xiaohongshu.com/explore/69396bc6000000000d03e473?note_flow_source=wechat&xsec_token=CB6G3F_yM99q8XfusvyRlJqm8Db4Es2k0kYIHdIUiSQ9g=)ã€‚

* [2025/05] AutoRound å·²é›†æˆè‡³ **Transformers**ã€‚è¯¦è§ï¼š[*åšå®¢*](https://huggingface.co/blog/autoround)ã€‚

* [2025/03] **DeepSeek-R1** æ¨¡å‹ï¼ˆçº¦ 200GBï¼‰åœ¨é‡åŒ–ï¼ˆä½¿ç”¨INT2-æ··åˆç²¾åº¦ï¼‰åä»ä¿æŒäº† 97.9% çš„å‡†ç¡®åº¦ã€‚è¯¦è§ï¼š[*æ¨¡å‹*](https://huggingface.co/OPEA/DeepSeek-R1-int2-mixed-sym-inc)ã€‚


## âœ¨ æ ¸å¿ƒç‰¹æ€§


âœ… **é«˜å‡†ç¡®åº¦** åœ¨ 2â€“3 bit ä¸‹ä¹Ÿèƒ½ä¿æŒè¾ƒå¼ºçš„æ€§èƒ½ï¼ˆ[ç¤ºä¾‹æ¨¡å‹](https://huggingface.co/collections/OPEA/2-3-bits-67a5f0bc6b49d73c01b4753b)ï¼‰ï¼Œ 4 bit é‡åŒ–åœ¨[åŸºå‡†](https://huggingface.co/spaces/Intel/low_bit_open_llm_leaderboard)ä¸Šä¿æŒé¢†å…ˆæ°´å¹³ã€‚

âœ… **è‰¯å¥½çš„ç”Ÿæ€é›†æˆ** é‡åŒ–æ¨¡å‹å·²è¢«å¤šä¸ªçŸ¥ååº“æ”¯æŒï¼Œå¦‚ **Transformersã€vLLMã€SGLang** ç­‰ã€‚

âœ… **å¤šæ ¼å¼å¯¼å‡º** å¯ä»¥å¯¼å‡ºåˆ°â€‹**AutoRoundã€AutoAWQã€AutoGPTQã€GGUF**â€‹ æ ¼å¼ï¼Œã€‚è¯¦è§ï¼š[å¯¼å‡ºæ ¼å¼](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#supported-export-formats)

âœ… **è‡ªåŠ¨æ··åˆç²¾åº¦** å¯åœ¨å‡ åˆ†é’Ÿå†…è‡ªåŠ¨ç”Ÿæˆæ··åˆbitç­–ç•¥ï¼Œä½†éœ€è¦æ¨¡å‹åœ¨ BF16ä¸‹å†…å­˜å ç”¨é‡çš„1.1â€“1.5å€ä½œä¸ºé¢å¤–å¼€é”€ã€‚è¯¦è§ï¼š[å‡†ç¡®åº¦ç»“æœ](https://github.com/intel/auto-round/blob/main/docs/auto_scheme_acc) å’Œ [ç”¨æˆ·æŒ‡å—](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme)

âœ… **ä¼˜åŒ–çš„å°±è¿‘å–æ•´ï¼ˆRTNï¼‰æ¨¡å¼** ä½¿ç”¨ `--iters 0`â€‹ å¯å¿«é€Ÿå®Œæˆé‡åŒ–ï¼ˆä½†åœ¨ 4 bit ä¸‹å‡†ç¡®åº¦ä¼šæœ‰ä¸€å®šé™ä½ï¼‰ã€‚è¯¦è§ï¼š[opt_rtn æ¨¡å¼](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#opt-rtn-mode)

âœ… **ä½é‡åŒ–æˆæœ¬** å•å¡ GPU ä¸Šé‡åŒ– 7B æ¨¡å‹ä»…éœ€çº¦ 10 åˆ†é’Ÿã€‚è¯¦è§ï¼š[é‡åŒ–æˆæœ¬](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#quantization-costs)

âœ… **æ”¯æŒ 10+ VLM æ¨¡å‹**  åä½™æ¬¾è§†è§‰-è¯­è¨€æ¨¡å‹å¼€ç®±å³ç”¨å¼é‡åŒ–ã€‚è¯¦è§ï¼š[ç¤ºä¾‹æ¨¡å‹](https://huggingface.co/collections/OPEA/vlms-autoround-675bc712fdd6a55ebaf11bfa)ï¼Œ[æ”¯æŒçŸ©é˜µ](https://github.com/intel/auto-round/tree/main/auto_round/mllm#support-matrix)

âœ… **å¤šç§é‡åŒ– Recipes** å¯é€‰ `auto-round-best`â€‹ã€`auto-round`â€‹ã€`auto-round-light`â€‹ ä»¥æ»¡è¶³ä¸åŒéœ€æ±‚ã€‚è¯¦è§ï¼š[é‡åŒ–Recipes](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#recipe-recommendation)

âœ… **é«˜çº§å·¥å…·é›†** æ”¯æŒ[å¤š GPU é‡åŒ–](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#devicemulti-gpu-setting-in-quantization)ã€[å¤šæ ‡å®šæ•°æ®é›†](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#default-dataset)ä»¥åŠ[åä½™ç§æ¨ç†åç«¯](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#specify-inference-backend)ã€‚

âœ… **ä¸æ­¢äºå•ä¸€æƒé‡é‡åŒ–** æ­£åœ¨ç§¯ææ‰©å±•æ›´å¤šæ•°æ®ç±»å‹çš„æ”¯æŒï¼ŒåŒ…æ‹¬ **MXFPã€NVFPã€W8A8** ç­‰ã€‚


## å®‰è£…

### ä» PyPI å®‰è£…

```shell
# CPU / Intel GPU / CUDA
pip install auto-round

# HPU
pip install auto-round-lib
```

<details>
  <summary>ä»æºç ç¼–è¯‘å®‰è£…</summary>

  ```bash
  # CPU/Intel GPU/CUDA
  pip install .

  # HPU
  python setup.py install lib
  ```

</details>

## æ¨¡å‹é‡åŒ–ï¼ˆCPU / Intel GPU / Gaudi / CUDAï¼‰

### CLI ç”¨æ³•

å®Œæ•´çš„å‚æ•°åˆ—è¡¨å¯é€šè¿‡åœ¨ç»ˆç«¯è¿è¡Œ `auto-round -h` æŸ¥çœ‹ã€‚

> **æ”¯æŒé€šè¿‡ ModelScope ä¸‹è½½æ¨¡å‹ï¼Œåªéœ€è®¾ç½®** â€‹**â€‹`AR_USE_MODELSCOPE=1`â€‹**ã€‚

```shell
auto-round \
    --model Qwen/Qwen3-0.6B \
    --scheme "W4A16" \
    --format "auto_round" \
    --output_dir ./tmp_autoround
```

æˆ‘ä»¬è¿˜æä¾›å¦å¤–ä¸¤ç§ recipesï¼š`auto-round-best`â€‹ï¼ˆè¿½æ±‚æœ€é«˜å‡†ç¡®åº¦ï¼‰å’Œ `auto-round-light`ï¼ˆè¿½æ±‚æ›´å¿«é€Ÿåº¦ï¼‰ï¼Œå…·ä½“å¦‚ä¸‹ï¼š


<details>
  <summary>å…¶ä»– Recipes</summary>

  ```bash
# æœ€ä½³å‡†ç¡®åº¦ï¼Œé€Ÿåº¦æ…¢ 3 å€ï¼Œlow_gpu_mem_usage å¯èŠ‚çœ ~20G æ˜¾å­˜ï¼Œä½†ä¼šæ…¢ ~30%
auto-round-best \
    --model Qwen/Qwen3-0.6B \
    --scheme "W4A16" \
    --low_gpu_mem_usage 
  ```

  ```bash
# 2â€“3 å€åŠ é€Ÿï¼ŒW4 ä¸‹å‡†ç¡®åº¦ç•¥é™ï¼ŒW2 ä¸‹å‡†ç¡®åº¦ä¸‹é™æ›´æ˜æ˜¾
auto-round-light \
    --model Qwen/Qwen3-0.6B \
    --scheme "W4A16" 
  ```

  <!-- ```bash
auto-round-fast \
# Fast and low memory, 2-3X speedup, slight accuracy drop at W4G128
    --model Qwen/Qwen3-0.6B \
    --bits 4 \
    --group_size 128 \
  ``` -->

</details> 

æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ â€‹**W4A16 åœºæ™¯ä¸‹ä½¿ç”¨ auto-roundï¼ŒW2A16 åœºæ™¯ä¸‹ä½¿ç”¨ auto-round-best å¹¶å¯ç”¨ â€‹`enable_alg_ext`â€‹â€‹** ã€‚å½“ç„¶ä½ ä¹Ÿå¯ä»¥æ ¹æ®è‡ªèº«éœ€æ±‚å’Œæ‰‹å¤´èµ„æºæ¥è‡ªè¡Œè°ƒæ•´é…ç½®ã€‚

### API ç”¨æ³•

```python
from auto_round import AutoRound

# åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒ FP8 / BF16 / FP16 / FP32ï¼‰
model_name_or_path = "Qwen/Qwen3-0.6B"

# å¯ç”¨ schemeï¼š"W2A16", "W3A16", "W4A16", "W8A16", "NVFP4", "MXFP4"ï¼ˆæ— çœŸå® kernelï¼‰, "GGUF:Q4_K_M" ç­‰
ar = AutoRound(model_name_or_path, scheme="W4A16")

# æœ€é«˜å‡†ç¡®åº¦ï¼ˆæ…¢ 4â€“5 å€ï¼‰
# `low_gpu_mem_usage=True` å¯èŠ‚çœ ~20GB æ˜¾å­˜ï¼Œä½†ä¼šæ…¢ ~30%
# ar = AutoRound(model_name_or_path, nsamples=512, iters=1000, low_gpu_mem_usage=True)

# æ›´å¿«é‡åŒ–ï¼ˆ2â€“3 å€åŠ é€Ÿï¼‰ï¼Œä½†åœ¨ W4G128 ä¸‹å‡†ç¡®åº¦ä¼šç•¥å¾®ä¸‹é™
# ar = AutoRound(model_name_or_path, nsamples=128, iters=50, lr=5e-3)

# æ”¯æŒæ ¼å¼ï¼š"auto_round"ï¼ˆé»˜è®¤ï¼‰, "auto_gptq", "auto_awq", "llm_compressor", "gguf:q4_k_m" ç­‰
ar.quantize_and_save(output_dir="./qmodel", format="auto_round")
```

<details>
<summary>æ ¸å¿ƒè¶…å‚æ•°è¯´æ˜</summary>

##### é‡åŒ–æ–¹æ¡ˆä¸é…ç½®

- â€‹**â€‹`scheme`â€‹**â€‹ï¼ˆstr | dict | AutoSchemeï¼‰ï¼šé¢„å®šä¹‰é‡åŒ–é”®ï¼Œå¦‚ `W4A16`â€‹ã€`MXFP4`â€‹ã€`NVFP4`â€‹ã€`GGUF:Q4_K_M`ã€‚å¯¹äº MXFP4/NVFP4ï¼Œæ¨èå¯¼å‡ºä¸º LLM-Compressor æ ¼å¼ã€‚
- â€‹**â€‹`bits`â€‹**â€‹ï¼ˆintï¼‰ï¼šé‡åŒ–æ¯”ç‰¹æ•°ï¼ˆé»˜è®¤ `None`ï¼‰ï¼Œéç©ºæ—¶ä¼šè¦†ç›– scheme è®¾ç½®ã€‚
- â€‹**â€‹`group_size`â€‹**â€‹ï¼ˆintï¼‰ï¼šé‡åŒ–åˆ†ç»„å¤§å°ï¼ˆé»˜è®¤ `None`ï¼‰ï¼Œéç©ºæ—¶ä¼šè¦†ç›– scheme è®¾ç½®ã€‚
- â€‹**â€‹`sym`â€‹**â€‹ï¼ˆboolï¼‰ï¼šæ˜¯å¦ä½¿ç”¨å¯¹ç§°é‡åŒ–ï¼ˆé»˜è®¤ `None`ï¼‰ï¼Œéç©ºæ—¶ä¼šè¦†ç›– scheme è®¾ç½®ã€‚
- â€‹**â€‹`layer_config`â€‹**â€‹ï¼ˆdictï¼‰ï¼šé€å±‚é‡åŒ–é…ç½®ï¼ˆé»˜è®¤ `None`ï¼‰ï¼Œä¸»è¦ç”¨äºè‡ªå®šä¹‰æ··åˆæ–¹æ¡ˆã€‚

##### ç®—æ³•ç›¸å…³è®¾ç½®

- â€‹**â€‹`enable_alg_ext`â€‹**â€‹ï¼ˆboolï¼‰ï¼š[å®éªŒæ€§åŠŸèƒ½] ä»…åœ¨ `iters > 0`â€‹ æ—¶ç”Ÿæ•ˆã€‚ä¸ºç‰¹å®š schemeï¼ˆå¦‚ MXFP4 / W2A16ï¼‰å¯ç”¨ç®—æ³•æ‰©å±•ï¼Œå¯èƒ½æ˜¾è‘—æå‡æ•ˆæœã€‚é»˜è®¤ `False`ã€‚
- â€‹**â€‹`disable_opt_rtn`â€‹**â€‹ï¼ˆbool | Noneï¼‰ï¼šå¯¹ç‰¹å®š schemeï¼ˆå¦‚ GGUF å’Œ WOQï¼‰ä½¿ç”¨çº¯ RTN æ¨¡å¼ã€‚é»˜è®¤ `None`â€‹ã€‚è‹¥ä¸º Noneï¼Œé€šå¸¸é»˜è®¤ä¸º `False`â€‹ ä»¥æå‡å‡†ç¡®åº¦ï¼Œä½†åœ¨å·²çŸ¥é—®é¢˜ä¸‹å¯èƒ½è®¾ä¸º `True`ã€‚

##### è®­ç»ƒå‚æ•°

- â€‹**â€‹`iters`â€‹**â€‹ï¼ˆintï¼‰ï¼šè°ƒå‚è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ `200`â€‹ï¼‰ã€‚å¸¸ç”¨å–å€¼ï¼š0ï¼ˆRTN æ¨¡å¼ï¼‰ã€50ï¼ˆæ¨è `lr=5e-3`ï¼‰ã€1000ã€‚è¿­ä»£æ¬¡æ•°è¶Šå¤šï¼Œå‡†ç¡®åº¦è¶Šé«˜ï¼Œä½†é€Ÿåº¦è¶Šæ…¢ã€‚
- â€‹**â€‹`lr`â€‹**â€‹ï¼ˆfloatï¼‰ï¼šèˆå…¥å€¼å­¦ä¹ ç‡ï¼ˆé»˜è®¤ `None`â€‹ï¼‰ã€‚è‹¥ä¸º Noneï¼Œåˆ™è‡ªåŠ¨è®¾ä¸º `1.0/iters`ã€‚
- â€‹**â€‹`batch_size`â€‹**â€‹ï¼ˆintï¼‰ï¼šè®­ç»ƒ batch sizeï¼ˆé»˜è®¤ `8`â€‹ï¼‰ï¼Œä¹Ÿå¸¸ç”¨ `4`ã€‚
- â€‹**â€‹`enable_deterministic_algorithms`â€‹**â€‹ï¼ˆboolï¼‰ï¼šæ˜¯å¦å¯ç”¨ç¡®å®šæ€§ç®—æ³•ä»¥ä¿è¯å¯å¤ç°æ€§ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚

##### æ ‡å®šæ•°æ®é›†

- â€‹**â€‹`dataset`â€‹**â€‹ï¼ˆstr | list | tuple | DataLoaderï¼‰ï¼šç”¨äºè°ƒå‚çš„æ•°æ®é›†ï¼ˆé»˜è®¤ `"NeelNanda/pile-10k"`â€‹ï¼‰ã€‚æ”¯æŒæœ¬åœ° JSON æ–‡ä»¶å’Œæ•°æ®é›†ç»„åˆï¼Œå¦‚ `"./tmp.json,NeelNanda/pile-10k:train,mbpp:train+validation+test"`ã€‚
- â€‹**â€‹`nsamples`â€‹**â€‹ï¼ˆintï¼‰ï¼šè°ƒå‚æ ·æœ¬æ•°ï¼ˆé»˜è®¤ `128`ï¼‰ã€‚
- â€‹**â€‹`seqlen`â€‹**â€‹ï¼ˆintï¼‰ï¼šè°ƒå‚åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ `2048`ï¼‰ã€‚

##### è®¾å¤‡ / é€Ÿåº¦é…ç½®

- â€‹**â€‹`enable_torch_compile`â€‹**ï¼ˆboolï¼‰ï¼šè‹¥æ— å¼‚å¸¸ï¼Œé€šå¸¸å»ºè®®å¼€å¯ä»¥è·å¾—æ›´å¿«çš„é‡åŒ–é€Ÿåº¦å’Œæ›´ä½èµ„æºæ¶ˆè€—ã€‚
- â€‹**â€‹`low_gpu_mem_usage`â€‹**â€‹ï¼ˆboolï¼‰ï¼šæ˜¯å¦å°†ä¸­é—´ç‰¹å¾å¸è½½åˆ° CPUï¼Œä»¥çº¦ 20% çš„æ—¶é—´ä»£ä»·èŠ‚çœæ˜¾å­˜ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚
- â€‹**â€‹`low_cpu_mem_usage`â€‹**â€‹ï¼ˆboolï¼‰ï¼š[å®éªŒæ€§åŠŸèƒ½] æ˜¯å¦å¯ç”¨å³æ—¶ä¿å­˜ä»¥å‡å°‘å†…å­˜å ç”¨ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚
- â€‹**â€‹`device_map`â€‹**â€‹ï¼ˆstr | dict | intï¼‰ï¼šè°ƒå‚ä½¿ç”¨çš„è®¾å¤‡ï¼Œå¦‚ `auto`â€‹ã€`cpu`â€‹ã€`cuda`â€‹ã€`0,1,2`â€‹ï¼ˆé»˜è®¤ `0`â€‹ï¼‰ã€‚ä½¿ç”¨ `auto` æ—¶ä¼šå°è¯•åˆ©ç”¨æ‰€æœ‰å¯ç”¨ GPUã€‚

</details>

### æ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆ
<details>
<summary>è¯¦ç»†è¯´æ˜</summary>
ç°è‰²è¡¨ç¤ºæ—  kernel æˆ–ä»…æœ‰ä½æ•ˆ/å‚è€ƒå®ç°ã€‚BF16 ä¸»è¦ç”¨äº AutoSchemeã€‚

|æ ¼å¼|æ”¯æŒçš„æ–¹æ¡ˆ|
| ------| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|**auto_round**|W4A16ï¼ˆæ¨èï¼‰ã€W2A16ã€W3A16ã€W8A16ã€W2A16G64ã€W2A16G32ã€`MXFP4`â€‹ã€`MXFP8`â€‹ã€`MXFP4_RCEIL`â€‹ã€`MXFP8_RCEIL`â€‹ã€`NVFP4`â€‹ã€`FPW8A16`â€‹ã€`FP8_STATIC`â€‹ã€`BF16`|
|**auto_awq**|W4A16ï¼ˆæ¨èï¼‰ã€BF16|
|**auto_gptq**|W4A16ï¼ˆæ¨èï¼‰ã€W2A16ã€W3A16ã€W8A16ã€W2A16G64ã€W2A16G32ã€BF16|
|**llm_compressor**|NVFP4ï¼ˆæ¨èï¼‰ã€`MXFP4`â€‹ã€`MXFP8`â€‹ã€`FPW8A16`â€‹ã€`FP8_STATIC`|
|**gguf**|GGUF:Q4\_K\_Mï¼ˆæ¨èï¼‰ã€Auto-RoundGGUF:Q2\_K\_Sã€GGUF:Q3\_K\_Sã€GGUF:Q3\_K\_Mã€GGUF:Q3\_K\_Lã€GGUF:Q4\_K\_Sã€GGUF:Q5\_K\_Sã€GGUF:Q5\_K\_Mã€GGUF:Q6\_Kã€GGUF:Q4\_0ã€GGUF:Q4\_1ã€GGUF:Q5\_0ã€GGUF:Q5\_1ã€GGUF:Q8\_0|
|**fake**|â€‹`æ‰€æœ‰æ–¹æ¡ˆï¼ˆä»…ç”¨äºç ”ç©¶ï¼‰`|
</details>

### è‡ªé€‚åº”é‡åŒ–ï¼ˆAutoSchemeï¼‰æ–¹æ¡ˆï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰

AutoScheme å†…ç½®è‡ªåŠ¨åŒ–ç®—æ³•ï¼Œå¯ç”Ÿæˆ **è‡ªé€‚åº”çš„æ··åˆä½å®½/æ•°æ®ç±»å‹** çš„é‡åŒ–recipeã€‚å…³äº AutoScheme çš„æ›´å¤šç»†èŠ‚å¯å‚è€ƒ[ç”¨æˆ·æŒ‡å—](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme)ã€‚

```python
from auto_round import AutoRound, AutoScheme

model_name = "Qwen/Qwen3-8B"
avg_bits = 3.0
scheme = AutoScheme(avg_bits=avg_bits, options=("GGUF:Q2_K_S", "GGUF:Q4_K_S"), ignore_scale_zp_bits=True)
layer_config = {"lm_head": "GGUF:Q6_K"}

# å¯¹äºé GGUF æ–¹æ¡ˆï¼Œå°† iters æ”¹ä¸º 200
ar = AutoRound(model=model_name, scheme=scheme, layer_config=layer_config, iters=0)
ar.quantize_and_save()
```

<details>
<summary>AutoScheme çš„é‡è¦è¶…å‚æ•°</summary>

##### AutoScheme è¶…å‚æ•°

- â€‹**â€‹`avg_bits`â€‹**â€‹  **(float)** ï¼šæ¨¡å‹æ•´ä½“ç›®æ ‡å¹³å‡ä½å®½ï¼Œä»…å°†é‡åŒ–å±‚çº³å…¥å¹³å‡ä½å®½çš„è®¡ç®—èŒƒå›´ã€‚
- â€‹**â€‹`options`â€‹**â€‹  **(str | list[str] | list[QuantizationScheme])** â€‹ï¼šé€‰å€™çš„é‡åŒ–æ–¹æ¡ˆé›†åˆï¼Œæ”¯æŒå•ä¸ªç”¨é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ `"W4A16,W2A16"`â€‹ï¼‰ã€å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä¾‹å¦‚ `["W4A16", "W2A16"]`â€‹ï¼‰æˆ– `QuantizationScheme` å¯¹è±¡åˆ—è¡¨ä¸‰ç§æ ¼å¼ã€‚
- â€‹**â€‹`ignore_scale_zp_bits`â€‹**â€‹  **(bool)** â€‹ï¼šä»…æ”¯æŒ API è°ƒç”¨åœºæ™¯ï¼Œç”¨äºå†³å®šåœ¨è®¡ç®—å¹³å‡ä½å®½æ—¶ï¼Œæ˜¯å¦æ’é™¤ scale ä¸ zero-point çš„æ¯”ç‰¹æ•°ï¼ˆé»˜è®¤ï¼š`False`ï¼‰ã€‚
- â€‹**â€‹`shared_layers`â€‹**â€‹  **(Iterable[Iterable[str]], optional)** ï¼šä»…æ”¯æŒ API è°ƒç”¨åœºæ™¯ï¼Œç”¨äºå®šä¹‰å…±äº«åŒä¸€é‡åŒ–è®¾ç½®çš„å±‚åˆ†ç»„ã€‚
- â€‹**â€‹`batch_size`â€‹**â€‹  **(int, optional)** â€‹ï¼šä»…æ”¯æŒ API è°ƒç”¨åœºæ™¯ï¼Œå¯è®¾ä¸º `1` ä»¥é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†ä¼šå¢åŠ è°ƒå‚æ—¶é—´ã€‚

</details>

### è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ API è°ƒç”¨æ–¹æ³•

è‹¥åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œå¯ä»¥å°è¯•è®¾ç½® `iters=0`â€‹ï¼ˆå¯ç”¨ RTNï¼‰å’Œ `group_size=32` æ¥æ”¹å–„æ•ˆæœã€‚


<details>
  <summary>ç‚¹å‡»å±•å¼€</summary>

**è¯¥åŠŸèƒ½ä¸ºå®éªŒæ€§åŠŸèƒ½ï¼Œåç»­å¯èƒ½ä¼šæœ‰æ”¹åŠ¨ã€‚**

é»˜è®¤æƒ…å†µä¸‹ï¼ŒAutoRound ä»…å¯¹ VLM çš„æ–‡æœ¬æ¨¡å—è¿›è¡Œé‡åŒ–ï¼Œä¸”é‡‡ç”¨ `NeelNanda/pile-10k`â€‹ ä½œä¸ºæ ¡å‡†æ•°æ®é›†ã€‚è‹¥è¦é‡åŒ–æ•´ä¸ªæ¨¡å‹ï¼Œå¯é€šè¿‡è®¾ç½® `quant_nontext_module`â€‹ ä¸º True å®ç°ï¼ˆä½†ç›®å‰è¯¥åŠŸèƒ½çš„æ”¯æŒèŒƒå›´æœ‰é™ï¼‰ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ AutoRound çš„ [readme] (https://github.com/intel/auto-round/blob/main/auto_round/mllm/README%7Creadme%5D%5D%E3%80%82)

```python
from auto_round import AutoRound

# åŠ è½½æ¨¡å‹
model_name_or_path = "Qwen/Qwen2.5-VL-7B-Instruct"
# é‡åŒ–æ¨¡å‹
ar = AutoRound(model_name_or_path, scheme="W4A16")
output_dir = "./qmodel"
ar.quantize_and_save(output_dir)
```

</details>



## æ¨¡å‹æ¨ç†

### vLLMï¼ˆCPU / Intel GPU / CUDAï¼‰

```python
from vllm import LLM, SamplingParams

prompts = [
    "Hello, my name is",
]
sampling_params = SamplingParams(temperature=0.6, top_p=0.95)
model_name = "Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound"
llm = LLM(model=model_name)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

### SGLangï¼ˆIntel GPU / CUDAï¼‰

**æ³¨æ„ï¼šå½“å‰å¯¹æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€ï¼ˆVLMï¼‰æ¨¡å‹çš„æ”¯æŒèŒƒå›´ä»ç„¶æœ‰é™ã€‚**

```python
import sglang as sgl

llm = sgl.Engine(model_path="Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound")
prompts = [
    "Hello, my name is",
]
sampling_params = {"temperature": 0.6, "top_p": 0.95}

outputs = llm.generate(prompts, sampling_params)
for prompt, output in zip(prompts, outputs):
    print(f"Prompt: {prompt}\nGenerated text: {output['text']}")
```

### Transformersï¼ˆCPU / Intel GPU / Gaudi / CUDAï¼‰

AutoRound æ”¯æŒåä½™ç§æ¨ç†åç«¯ï¼Œå¹¶ä¼šæ ¹æ®å·²å®‰è£…çš„åº“è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¯ç”¨åç«¯ï¼›è‹¥æ£€æµ‹åˆ°æ›´ä¼˜åç«¯ä½†ç¼ºå°‘ç›¸å…³ä¾èµ–æ—¶ï¼Œä¹Ÿä¼šæç¤ºç”¨æˆ·å®‰è£…é¢å¤–åº“ã€‚

â€‹**æ¨ç†è¿‡ç¨‹ä¸­è¯·é¿å…æ‰‹åŠ¨å°†é‡åŒ–åçš„æ¨¡å‹è¿ç§»åˆ°å…¶ä»–è®¾å¤‡**â€‹ï¼ˆä¾‹å¦‚æ‰§è¡Œ `model.to('cpu')`ï¼‰ï¼Œå¦åˆ™å¯èƒ½å¼•å‘æœªçŸ¥å¼‚å¸¸ã€‚

ç›®å‰å¯¹ Gaudi è®¾å¤‡çš„æ”¯æŒè¾ƒä¸ºæœ‰é™ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "There is a girl who likes adventure,"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50)[0]))
```

## ç ”ç©¶æˆæœ & å…¶ä»–æ´»åŠ¨

[SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs](https://arxiv.org/abs/2512.04746)ï¼ˆ202512 è®ºæ–‡ï¼‰

[Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLM](https://aclanthology.org/2024.findings-emnlp.662/)ï¼ˆ202309 è®ºæ–‡ï¼‰

[TEQ: Trainable Equivalent Transformation for Quantization of LLMs](https://arxiv.org/abs/2310.10944)ï¼ˆ202310 è®ºæ–‡ï¼‰

[Effective Post-Training Quantization for Large Language Models](https://medium.com/intel-analytics-software/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98)ï¼ˆ202304 åšå®¢ï¼‰

æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹ [å®Œæ•´è®ºæ–‡åˆ—è¡¨](./docs/publication_list.md).

## è‡´è°¢

ç‰¹åˆ«æ„Ÿè°¢ AutoGPTQã€AutoAWQã€GPTQModelã€Tritonã€Marlinã€ExLLaMAV2 ç­‰å¼€æº low-precision åº“æä¾›ä½ç²¾åº¦ CUDA kernelï¼Œåœ¨æ­¤åŸºç¡€ä¸Š AutoRound é¡¹ç›®ä½œäº†åˆ©ç”¨ä¸é›†æˆã€‚

## ğŸŒŸ æ”¯æŒæˆ‘ä»¬

å¦‚æœè§‰å¾— AutoRound å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç»™ä»“åº“ç‚¹ä¸ª â­ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„ç¤¾åŒºï¼

