<div align="center">



<p align="center">
  <img src="docs/imgs/AutoRound.png" alt="AutoRound Overview" width="20%">
</p>


<h3> é¢å‘ LLM çš„é«˜çº§é‡åŒ–ç®—æ³•</h3>

[![python](https://img.shields.io/badge/python-3.10%2B-blue)](https://github.com/intel/auto-round)
[![version](https://img.shields.io/badge/release-0.9.5-green)](https://github.com/intel/auto-round)
[![license](https://img.shields.io/badge/license-Apache%202-9C27B0)](https://github.com/intel/auto-round/blob/main/LICENSE)
<a href="https://huggingface.co/Intel">
<img alt="Model Checkpoints" src="https://img.shields.io/badge/%F0%9F%A4%97%20HF-Models-F57C00">
</a>

[English](README.md) | ç®€ä½“ä¸­æ–‡

---
<div align="left">

## ğŸš€ AutoRound æ˜¯ä»€ä¹ˆï¼Ÿ

AutoRound æ˜¯ä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è®¾è®¡çš„é«˜çº§é‡åŒ–å·¥å…·åŒ…ã€‚å®ƒèƒ½åœ¨ **æä½æ¯”ç‰¹ï¼ˆ2â€“4 bitsï¼‰** ä¸‹å®ç°è¾ƒé«˜çš„æ¨¡å‹ç²¾åº¦ï¼Œæ‰€éœ€è°ƒå‚æå°‘ã€‚å…¶æ ¸å¿ƒæ˜¯ **ç¬¦å·æ¢¯åº¦ä¸‹é™ï¼ˆsign-gradient descentï¼‰æ³•**ã€‚æ­¤å¤–ï¼Œè¯¥å·¥å…·è¿˜å…·å¤‡è‰¯å¥½çš„ç¡¬ä»¶å…¼å®¹æ€§ã€‚æ›´å¤šç»†èŠ‚è¯¦è§è®ºæ–‡ [SignRoundV1](https://arxiv.org/pdf/2309.05516) å’Œ [SignRoundV2](http://arxiv.org/abs/2512.04746)ã€‚ä½¿ç”¨æ–¹æ³•è¯·å‚é˜… [ç”¨æˆ·æŒ‡å—](./docs/step_by_step.md).

<p align="center">
  <img src="docs/imgs/autoround_overview.png" alt="AutoRound Overview" width="80%">
</p>


## ğŸ†• æœ€æ–°è¿›å±•

* [2025/12] å‘å¸ƒ **SignRoundV2** è®ºæ–‡ã€‚è¦å¤ç°è®ºæ–‡æˆæœï¼Œå¯å¯ç”¨ `enable_alg_ext`ï¼Œå¹¶ä½¿ç”¨ **AutoScheme** API å¯¹æ¨¡å‹è¿›è¡Œæ··åˆç²¾åº¦é‡åŒ–ã€‚ç›¸å…³é“¾æ¥ï¼š[*è®ºæ–‡*](http://arxiv.org/abs/2512.04746)ï¼Œ[*LLaMA æ¨¡å‹è¯„ä¼°è¯´æ˜*](./docs/alg_202508.md)ã€‚

* [2025/11] **LLM-Compressor** å·²æ”¯æŒ AutoRound ç®—æ³•ã€‚ç›¸å…³é“¾æ¥ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://github.com/vllm-project/llm-compressor/tree/main/examples/autoround/README.md)ï¼Œ[*vLLM åšå®¢*](https://blog.vllm.ai/2025/12/09/intel-autoround-llmc.html)ï¼Œ[*RedHat åšå®¢*](https://developers.redhat.com/articles/2025/12/09/advancing-low-bit-quantization-llms-autoround-x-llm-compressor)ï¼Œ[*X æ¨æ–‡*](https://x.com/vllm_project/status/1998710451312771532)ï¼Œ[*Intel åšå®¢*](https://community.intel.com/t5/Blogs/Products-and-Solutions/HPC/Advancing-Low-Bit-Quantization-for-LLMs-AutoRound-x-LLM/post/1729336)ï¼Œ[*LinkedIn*](https://www.linkedin.com/posts/vllm-project_advancing-lowbit-quantization-for-llms-activity-7404478053768441856-ru8f/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAapNW8BLnAdCAr57GOwSCJXjf76ZvOEOAg)ï¼Œ[*å¾®ä¿¡*](https://mp.weixin.qq.com/s/l5WA-1_4ipffQN6GOH2Iqg)ï¼Œ[*çŸ¥ä¹*](https://zhuanlan.zhihu.com/p/1982167638315664412)ã€‚

* [2025/11] æ¨å‡º **å¢å¼ºç‰ˆ GGUF** é‡åŒ–ç®—æ³•ï¼Œå¯ç”¨ `--enable_alg_ext` å³å¯ã€‚ç›¸å…³é“¾æ¥ï¼š[*Accuracy*](./docs/gguf_alg_ext_acc.md)ã€‚

* [2025/10] **SGLang** å·²é›†æˆ AutoRoundã€‚ç›¸å…³é“¾æ¥ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://docs.sglang.io/advanced_features/quantization.html#using-auto-round)ï¼Œ[*LMSYS åšå®¢*](https://lmsys.org/blog/2025-11-13-AutoRound/)ï¼Œ[*X æ¨æ–‡*](https://x.com/lmsysorg/status/1991977019220148650?s=20)ï¼Œ[*Intel åšå®¢*](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/AutoRound-Meets-SGLang-Enabling-Quantized-Model-Inference-with/post/1727196)ï¼Œ[*LinkedIn*](https://www.linkedin.com/feed/update/urn:li:activity:7397742859354857472)ã€‚

* [2025/10] æ¨å‡º **æ··åˆç²¾åº¦** ç®—æ³•ï¼Œå¯åœ¨å‡ åˆ†é’Ÿå†…è‡ªåŠ¨ç”Ÿæˆé‡åŒ–æ–¹æ¡ˆã€‚ç›¸å…³é“¾æ¥ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme)ï¼Œ[*Accuracy*](./docs/auto_scheme_acc.md)ã€‚

* [2025/09] æ–°å¢å¯¹ **MXFP4** å’Œ **NVFP4** æ•°æ®ç±»å‹çš„æ”¯æŒã€‚ç›¸å…³é“¾æ¥ï¼š[*Accuracy*](./docs/mxnv_acc.md)ã€‚

* [2025/08] æä¾› **æ”¹è¿›ç‰ˆ INT2** ç®—æ³•ï¼Œå¯ç”¨ `--enable_alg_ext` å³å¯ã€‚ç›¸å…³é“¾æ¥ï¼š[*Accuracy*](./docs/alg_202508.md)ã€‚

* [2025/07] æ–°å¢ **GGUF** æ ¼å¼å¯¼å‡ºã€‚ç›¸å…³é“¾æ¥ï¼š[*ä½¿ç”¨æ–¹æ³•*](./docs/step_by_step.md#gguf-format)ã€‚

* [2025/05]  **vLLM** ç°å·²é›†æˆ AutoRoundã€‚ç›¸å…³é“¾æ¥ï¼š[*ä½¿ç”¨æ–¹æ³•*](https://docs.vllm.ai/en/latest/features/quantization/auto_round/)ï¼Œ[*Medium åšå®¢*](https://medium.com/@NeuralCompressor/accelerating-vllm-and-sglang-deployment-using-autoround-45fdc0b2683e)ï¼Œ[*å°çº¢ä¹¦*](https://www.xiaohongshu.com/explore/69396bc6000000000d03e473?note_flow_source=wechat&xsec_token=CB6G3F_yM99q8XfusvyRlJqm8Db4Es2k0kYIHdIUiSQ9g=)ã€‚

* [2025/05] **Transformers** å·²é›†æˆ AutoRoundã€‚ç›¸å…³é“¾æ¥ï¼š[*åšå®¢*](https://huggingface.co/blog/autoround)ã€‚

* [2025/03] çº¦ 200GB çš„ **DeepSeek-R1** æ¨¡å‹ç»é‡åŒ–ï¼ˆINT2æ··åˆç²¾åº¦ï¼‰åç²¾åº¦ä»é«˜è¾¾ 97.9%ã€‚ç›¸å…³é“¾æ¥ï¼š[*æ¨¡å‹*](https://huggingface.co/OPEA/DeepSeek-R1-int2-mixed-sym-inc)ã€‚


## âœ¨ æ ¸å¿ƒç‰¹æ€§


âœ… **æ¨¡å‹ç²¾åº¦å“è¶Š** åœ¨ 2â€“3 bit çš„æä½ç²¾åº¦ä¸‹ï¼Œæ¨¡å‹ä¹Ÿèƒ½ä¿æŒå¼ºåŠ²æ€§èƒ½ï¼ˆ[ç¤ºä¾‹æ¨¡å‹](https://huggingface.co/collections/OPEA/2-3-bits-67a5f0bc6b49d73c01b4753b)ï¼‰ï¼›åœ¨ 4 bit é‡åŒ–ä¸Šï¼Œæ¨¡å‹çš„[åŸºå‡†æµ‹è¯•](https://huggingface.co/spaces/Intel/low_bit_open_llm_leaderboard)æˆç»©ä¹Ÿå¤„åœ¨é¢†å…ˆæ°´å¹³ã€‚

âœ… **ç”Ÿæ€é›†æˆåº¦å¥½** ä¸ **Transformersã€vLLMã€SGLang** ç­‰ä¸»æµæ¡†æ¶æ— ç¼è¡”æ¥ã€‚

âœ… **å¯¼å‡ºæ ¼å¼ä¸°å¯Œ** æ”¯æŒå¯¼å‡ºä¸º â€‹**AutoRoundã€AutoAWQã€AutoGPTQ** åŠ **GGUF**â€‹ æ ¼å¼ï¼Œå…·å¤‡å‡ºè‰²çš„å…¼å®¹æ€§ã€‚ç›¸å…³é“¾æ¥ï¼š[å¯¼å‡ºæ ¼å¼](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#supported-export-formats)

âœ… **è‡ªåŠ¨æ··åˆç²¾åº¦** å¯åœ¨å‡ åˆ†é’Ÿå†…è‡ªåŠ¨ç”Ÿæˆæ··åˆ bit ç­–ç•¥ï¼ˆä½†éœ€è¦é¢å¤–å ç”¨æ¨¡å‹åœ¨ BF16 æ ¼å¼ä¸‹å†…å­˜å ç”¨é‡çš„ 1.1-1.5 å€ï¼‰ã€‚è¯¦è§ï¼šAccuracy [ç»“æœ](https://github.com/intel/auto-round/blob/main/docs/auto_scheme_acc.md) å’Œ [ç”¨æˆ·æŒ‡å—](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme)

âœ… **ä¼˜åŒ–çš„ RTN æ¨¡å¼** ä½¿ç”¨ `--iters 0`â€‹ å‚æ•°å¯å¯ç”¨ä¼˜åŒ–çš„ Round-to-Nears æ¨¡å¼ï¼Œå®ç°å¿«é€Ÿé‡åŒ–ï¼ˆä½†åœ¨ 4 bit ä¸‹å‡†ç¡®åº¦ä¼šæœ‰ä¸€å®šé™ä½ï¼‰ã€‚è¯¦è§ï¼š[opt_rtn æ¨¡å¼](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#opt-rtn-mode)ã€‚

âœ… **å¯æ¥å—çš„é‡åŒ–æˆæœ¬** åœ¨å•å¼  GPU ä¸Šé‡åŒ–ä¸€ä¸ª 7B çš„æ¨¡å‹åªéœ€çº¦ååˆ†é’Ÿã€‚è¯¦è§ï¼š[é‡åŒ–æˆæœ¬](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#quantization-costs)

âœ… **æ”¯æŒåä½™ç§ VLM æ¨¡å‹**  å¯¹åä½™ç§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ”¯æŒï¼Œè®©ç”¨æˆ·æœ‰â€œå¼€ç›–å³é£Ÿâ€èˆ¬çš„é‡åŒ–ä½“éªŒã€‚è¯¦è§ï¼š[ç¤ºä¾‹æ¨¡å‹](https://huggingface.co/collections/OPEA/vlms-autoround-675bc712fdd6a55ebaf11bfa)ï¼Œ[æ”¯æŒçŸ©é˜µ](https://github.com/intel/auto-round/tree/main/auto_round/mllm#support-matrix)

âœ… **å¤šç§é‡åŒ–æ–¹æ¡ˆå¯é€‰**  æä¾›`auto-round-best`â€‹ã€`auto-round`â€‹ã€`auto-round-light`â€‹ ç­‰å¤šç§é¢„è®¾æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ»¡è¶³å¤šæ ·åŒ–éœ€æ±‚ã€‚è¯¦è§ï¼š[é‡åŒ–æ–¹æ¡ˆ](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#recipe-recommendation)

âœ… **å®ç”¨é¢å¤–ç‰¹æ€§** æ”¯æŒ[å¤š GPU é‡åŒ–](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#devicemulti-gpu-setting-in-quantization)å’Œ[å¤šæ ‡å®šæ•°æ®é›†](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#default-dataset)ï¼Œå¹¶å…¼å®¹[åä½™ç§æ¨ç†åç«¯](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#specify-inference-backend)ã€‚

âœ… **ä¸å±€é™äºæƒé‡é‡åŒ–** æˆ‘ä»¬æ­£ç§¯ææ‰©å±•å¯¹ **MXFPã€NVFPã€W8A8** ç­‰æ›´å¤šæ•°æ®ç±»å‹çš„æ”¯æŒã€‚


## å®‰è£…

### ä» PyPI å®‰è£…

```shell
# CPU / Intel GPU / CUDA
pip install auto-round

# HPU
pip install auto-round-lib
```

<details>
  <summary>ä»æºç ç¼–è¯‘å®‰è£…</summary>

  ```bash
  # CPU/Intel GPU/CUDA
  pip install .

  # HPU
  python setup.py install lib
  ```

</details>

## æ¨¡å‹é‡åŒ–ï¼ˆCPU / Intel GPU / Gaudi / CUDAï¼‰

### CLI ç”¨æ³•

ç»ˆç«¯è¿è¡Œ `auto-round -h` å¯ä»¥æŸ¥çœ‹ auto-round å®Œæ•´çš„å‚æ•°åˆ—è¡¨ã€‚

> **æ”¯æŒé€šè¿‡ ModelScope ä¸‹è½½æ¨¡å‹ï¼ˆåªéœ€è®¾ç½®** â€‹**â€‹`AR_USE_MODELSCOPE=1`â€‹** ï¼‰ã€‚

```shell
auto-round \
    --model Qwen/Qwen3-0.6B \
    --scheme "W4A16" \
    --format "auto_round" \
    --output_dir ./tmp_autoround
```

å¦å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›`auto-round-best`â€‹ å’Œ `auto-round-light`ä¸¤ç§æ–¹æ¡ˆï¼Œå‰è€…æ—¨åœ¨è¿½æ±‚æ›´é«˜çš„æ¨¡å‹ç²¾åº¦ï¼Œåè€…åˆ™ä¸“æ³¨äºæå‡é‡åŒ–é€Ÿåº¦ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹ï¼š


<details>
  <summary>å…¶ä»–æ–¹æ¡ˆ</summary>

  ```bash
# æœ€ä½³ç²¾åº¦ï¼Œé€Ÿåº¦æ…¢ 3 å€ï¼Œlow_gpu_mem_usage å¯èŠ‚çœ ~20G æ˜¾å­˜ï¼Œä½†ä¼šæ…¢ ~30%
auto-round-best \
    --model Qwen/Qwen3-0.6B \
    --scheme "W4A16" \
    --low_gpu_mem_usage
  ```

  ```bash
# 2â€“3 å€åŠ é€Ÿï¼ŒW4 ä¸‹å‡†ç¡®åº¦ç•¥é™ï¼ŒW2 ä¸‹å‡†ç¡®åº¦ä¸‹é™æ›´æ˜æ˜¾
auto-round-light \
    --model Qwen/Qwen3-0.6B \
    --scheme "W4A16"
  ```

  <!-- ```bash
auto-round-fast \
# Fast and low memory, 2-3X speedup, slight accuracy drop at W4G128
    --model Qwen/Qwen3-0.6B \
    --bits 4 \
    --group_size 128 \
  ``` -->

</details>

å°ç»“ï¼šå¯¹äº â€‹**W4A16 é‡åŒ–ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨é»˜è®¤çš„ auto-roundï¼›è€Œ W2A16 é‡åŒ–æˆ‘ä»¬åˆ™æ¨èå¯ç”¨ â€‹`enable_alg_ext`â€‹ å‚æ•°çš„ auto-round-bestâ€‹**ã€‚å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ ¹æ®è‡ªèº«éœ€æ±‚å’Œæ‰‹å¤´ä¸Šçš„ç®—åŠ›çµæ´»è°ƒæ•´é…ç½®ã€‚

### API ç”¨æ³•

```python
from auto_round import AutoRound

# åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒ FP8 / BF16 / FP16 / FP32ï¼‰
model_name_or_path = "Qwen/Qwen3-0.6B"

# å¯é€‰é‡åŒ–é…ç½®ï¼š
# "W2A16", "W3A16", "W4A16", "W8A16", "NVFP4", "MXFP4"ï¼ˆæ— çœŸå® kernelï¼‰, "GGUF:Q4_K_M" ç­‰
ar = AutoRound(model_name_or_path, scheme="W4A16")

# è¿½æ±‚é«˜æ¨¡å‹ç²¾åº¦ï¼ˆé€Ÿåº¦ä¼šæ…¢ 4â€“5 å€ï¼‰
# `low_gpu_mem_usage=True` å¯èŠ‚çœ ~20GB æ˜¾å­˜ï¼Œä½†ä¼šæ…¢ ~30%
# ar = AutoRound(model_name_or_path, nsamples=512, iters=1000, low_gpu_mem_usage=True)

# è¿½æ±‚é‡åŒ–é€Ÿåº¦ï¼ˆæé€Ÿ 2â€“3 å€ï¼‰ï¼Œä½†åœ¨ W4G128 ä¸‹ç²¾åº¦ä¼šç•¥å¾®ä¸‹é™
# ar = AutoRound(model_name_or_path, nsamples=128, iters=50, lr=5e-3)

# æ”¯æŒçš„å¯¼å‡ºæ ¼å¼ï¼š"auto_round"ï¼ˆé»˜è®¤ï¼‰, "auto_gptq", "auto_awq", "llm_compressor", "gguf:q4_k_m" ç­‰
ar.quantize_and_save(output_dir="./qmodel", format="auto_round")
```

<details>
<summary>æ ¸å¿ƒè¶…å‚æ•°</summary>

##### é‡åŒ–æ–¹æ¡ˆ & é…ç½®

- â€‹**â€‹`scheme`â€‹**â€‹ï¼ˆstr | dict | AutoSchemeï¼‰ï¼šé¢„å®šä¹‰çš„å¦‚ `W4A16`â€‹ã€`MXFP4`â€‹ã€`NVFP4`â€‹ã€`GGUF:Q4_K_M`ç­‰é‡åŒ–é…ç½®æ ‡è¯†ã€‚å…¶ä¸­å¯¹äº MXFP4/NVFP4 æ–¹æ¡ˆï¼Œæˆ‘ä»¬æ¨èå¯¼å‡ºä¸º LLM-Compressor æ ¼å¼ã€‚
- â€‹**â€‹`bits`â€‹**â€‹ï¼ˆintï¼‰ï¼šé‡åŒ–ç›®æ ‡ç²¾åº¦ï¼ˆé»˜è®¤å€¼ä¸º `None`ï¼‰ã€‚è‹¥æŒ‡å®šæ­¤å‚æ•°ï¼Œå°†è¦†ç›– scheme ä¸­çš„è®¾ç½®ã€‚
- â€‹**â€‹`group_size`â€‹**â€‹ï¼ˆintï¼‰ï¼šé‡åŒ–åˆ†ç»„å¤§å°ï¼ˆé»˜è®¤å€¼ä¸º `None`ï¼‰ã€‚è‹¥æŒ‡å®šæ­¤å‚æ•°ï¼Œå°†è¦†ç›– scheme ä¸­çš„è®¾ç½®ã€‚
- â€‹**â€‹`sym`â€‹**â€‹ï¼ˆboolï¼‰ï¼šæ˜¯å¦ä½¿ç”¨å¯¹ç§°é‡åŒ–ï¼ˆé»˜è®¤å€¼ä¸º `None`ï¼‰ã€‚è‹¥æŒ‡å®šæ­¤å‚æ•°ï¼Œå°†è¦†ç›– scheme ä¸­çš„è®¾ç½®ã€‚
- â€‹**â€‹`layer_config`â€‹**â€‹ï¼ˆdictï¼‰ï¼šå±‚çº§è‡ªå®šä¹‰é…ç½®ï¼ˆé»˜è®¤å€¼ä¸º `None`ï¼‰ã€‚ä¸»è¦ç”¨äºè‡ªå®šä¹‰æ··åˆåŒ–æ–¹æ¡ˆï¼Œå¯ä»¥å¯¹æ¯ä¸€å±‚è®¾ç½®ä¸“é—¨çš„é‡åŒ–å‚æ•°ã€‚

##### ç®—æ³•ç›¸å…³è®¾ç½®

- â€‹**â€‹`enable_alg_ext`â€‹**â€‹ï¼ˆboolï¼‰ï¼š[å®éªŒæ€§åŠŸèƒ½] ä»…åœ¨ `iters > 0`â€‹ æ—¶ç”Ÿæ•ˆã€‚åœ¨ç‰¹å®š schemeï¼ˆå¦‚ MXFP4 / W2A16ï¼‰ä¸‹å¯ç”¨ç®—æ³•æ‰©å±•ï¼Œå¯æ˜¾è‘—æå‡é‡åŒ–æ•ˆæœã€‚é»˜è®¤å€¼ä¸º `False`ã€‚
- â€‹**â€‹`disable_opt_rtn`â€‹**â€‹ï¼ˆbool | Noneï¼‰ï¼šæ˜¯å¦å¯¹ç‰¹å®šæ–¹æ¡ˆï¼ˆå¦‚ GGUF ä¸æƒé‡é‡åŒ–æ–¹æ¡ˆï¼‰ç¦ç”¨ä¼˜åŒ–çš„ RTN æ¨¡å¼ã€‚ä¼˜åŒ–çš„ RTN æ¨¡å¼éœ€è¦æ ‡å®šæ•°æ®å’Œæ›´å¤šçš„ç®—åŠ›æ¥æå‡ç²¾åº¦ã€‚é»˜è®¤å€¼ä¸º `None`ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä¸ºæå‡ç²¾åº¦ï¼Œç®—æ³•ä¼šè‡ªåŠ¨é‡‡ç”¨ä¼˜åŒ–çš„ RTN æ¨¡å¼ï¼ˆå³ `False`ï¼‰ï¼›ä»…åœ¨å·²çŸ¥å­˜åœ¨å…¼å®¹æ€§é—®é¢˜æ—¶ï¼Œæ‰ä¼šè‡ªåŠ¨ç¦ç”¨ï¼ˆå³ `True`ï¼‰



##### è®­ç»ƒå‚æ•°

- â€‹**â€‹`iters`â€‹**â€‹ï¼ˆintï¼‰ï¼šè®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼ˆtuning iterationsï¼‰ï¼ˆé»˜è®¤å€¼ä¸º `200`â€‹ï¼‰ã€‚å¸¸ç”¨å–å€¼ï¼š0ï¼ˆRTN æ¨¡å¼ï¼‰ã€50ï¼ˆæ¨èæ­é… `lr=5e-3`ï¼‰ã€1000ï¼ˆæ›´é«˜ç²¾åº¦ä½†é‡åŒ–é€Ÿåº¦æ…¢ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´è¿­ä»£æ¬¡æ•°è¶Šå¤šï¼Œå‡†ç¡®åº¦è¶Šé«˜ï¼Œä½†é€Ÿåº¦è¶Šæ…¢ã€‚
- â€‹**â€‹`lr`â€‹**â€‹ï¼ˆfloatï¼‰ï¼šèˆå…¥å€¼ï¼ˆrounding rateï¼‰çš„å­¦ä¹ ç‡ï¼ˆé»˜è®¤å€¼ä¸º `None`â€‹ï¼‰ã€‚å½“ä¸º None æ—¶ï¼Œå°†è‡ªåŠ¨è®¾ä¸º `1.0/iters`ã€‚
- â€‹**â€‹`batch_size`â€‹**â€‹ï¼ˆintï¼‰ï¼šè®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰ã€‚é»˜è®¤ä¸º `8`â€‹ï¼Œä¹Ÿå¸¸ç”¨ `4`ã€‚
- â€‹**â€‹`enable_deterministic_algorithms`â€‹**â€‹ï¼ˆboolï¼‰ï¼šè‹¥æƒ³ä¿è¯ç»“æœå¯ä»¥å¤ç°ï¼Œå¯ä»¥è®¾ä¸º `True` æ¥å¯ç”¨ç¡®å®šæ€§ç®—æ³•ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚

##### æ ‡å®šæ•°æ®é›†

- â€‹**â€‹`dataset`â€‹**â€‹ï¼ˆstr | list | tuple | DataLoaderï¼‰ï¼šé‡åŒ–ä¸­ç”¨äºæ ¡å‡†çš„æ•°æ®é›†ï¼ˆé»˜è®¤ `"NeelNanda/pile-10k"`â€‹ï¼‰ã€‚æ”¯æŒæœ¬åœ° JSON æ–‡ä»¶å’Œæ•°æ®é›†ç»„åˆä½¿ç”¨ï¼Œå¦‚ `"./tmp.json,NeelNanda/pile-10k:train,mbpp:train+validation+test"`ã€‚
- â€‹**â€‹`nsamples`â€‹**â€‹ï¼ˆintï¼‰ï¼šæ ¡å‡†æ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤ `128`ï¼‰ã€‚
- â€‹**â€‹`seqlen`â€‹**â€‹ï¼ˆintï¼‰ï¼šæ¯æ¡æ ·æœ¬åœ¨æ ¡å‡†æ—¶ä½¿ç”¨ token çš„åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ `2048`ï¼‰ã€‚

##### è®¾å¤‡ / é€Ÿåº¦é…ç½®

- â€‹**â€‹`enable_torch_compile`â€‹**ï¼ˆboolï¼‰ï¼šé€šå¸¸å»ºè®®è®¾ä¸º `True` æ¥æå‡é‡åŒ–é€Ÿåº¦ã€é™ä½èµ„æºæ¶ˆè€—ï¼Œä½†æ˜¯æœ‰æå°æ¦‚ç‡ä¼šè§¦å‘å¼‚å¸¸ï¼Œå»ºè®®ä½¿ç”¨æœ€æ–°çš„ tiron ç‰ˆæœ¬ã€‚
- â€‹**â€‹`low_gpu_mem_usage`â€‹**â€‹ï¼ˆboolï¼‰ï¼šè‹¥è¦èŠ‚çœæ˜¾å­˜ï¼Œå¯ä»¥è®¾ä¸º `True` ã€‚å®ƒä¼šå°†ä¸­é—´ç‰¹å¾å¸è½½åˆ° CPUï¼Œä½†ä¼šå¢åŠ  20% çš„æ—¶é—´ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚
- â€‹**â€‹`low_cpu_mem_usage`â€‹**â€‹ï¼ˆboolï¼‰ï¼š[å®éªŒæ€§åŠŸèƒ½] è‹¥è¦å‡å°‘å†…å­˜å ç”¨ï¼Œå¯ä»¥è®¾ä¸º `True` æ¥å¯ç”¨å³æ—¶ä¿å­˜ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚
- â€‹**â€‹`device_map`â€‹**â€‹ï¼ˆstr | dict | intï¼‰ï¼šè®¡ç®—è®¾å¤‡æŒ‡å®šï¼Œå¦‚ `auto`â€‹ã€`cpu`â€‹ã€`cuda`â€‹ã€`0,1,2`â€‹ï¼ˆé»˜è®¤ `0`â€‹ï¼‰ã€‚ä½¿ç”¨ `auto` æ—¶ä¼šå°è¯•åˆ©ç”¨æ‰€æœ‰å¯ç”¨ GPUã€‚

</details>

### æ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆ
<details>
<summary>è¯¦ç»†è¯´æ˜</summary>
å¯ä»¥çœ‹åˆ°æœ‰äº› schemes ä¸ºç°è‰²èƒŒæ™¯ï¼Œè¿™é€šå¸¸è¡¨ç¤ºå®ƒæ²¡æœ‰ä¸“é—¨ä¼˜åŒ–çš„å†…æ ¸ï¼Œæˆ–åªæœ‰æ•ˆç‡æä½çš„å‚è€ƒå†…æ ¸ã€‚
å…¶ä¸­ï¼Œ BF16 ä¸»è¦é€‚ç”¨äº AutoSchemeï¼ˆå…¶ä»–æ–¹æ¡ˆä¸€èˆ¬ä¸ç”¨ï¼‰ã€‚

|æ ¼å¼| æ”¯æŒçš„scheme                                                                                                                                                                                       |
| ------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|**auto_round**| W4A16ï¼ˆæ¨èï¼‰ã€W2A16ã€W3A16ã€W8A16ã€W2A16G64ã€W2A16G32ã€`MXFP4`â€‹ã€`MXFP8`â€‹ã€`MXFP4_RCEIL`â€‹ã€`MXFP8_RCEIL`â€‹ã€`NVFP4`â€‹ã€`FPW8A16`â€‹ã€`FP8_STATIC`â€‹ã€`BF16`                                                          |
|**auto_awq**| W4A16ï¼ˆæ¨èï¼‰ã€BF16                                                                                                                                                                                  |
|**auto_gptq**| W4A16ï¼ˆæ¨èï¼‰ã€W2A16ã€W3A16ã€W8A16ã€W2A16G64ã€W2A16G32ã€BF16                                                                                                                                              |
|**llm_compressor**| NVFP4ï¼ˆæ¨èï¼‰ã€`MXFP4`â€‹ã€`MXFP8`â€‹ã€`FPW8A16`â€‹ã€`FP8_STATIC`                                                                                                                                             |
|**gguf**| GGUF:Q4\_K\_Mï¼ˆæ¨èï¼‰ã€Auto-RoundGGUF:Q2\_K\_Sã€GGUF:Q3\_K\_Sã€GGUF:Q3\_K\_Mã€GGUF:Q3\_K\_Lã€GGUF:Q4\_K\_Sã€GGUF:Q5\_K\_Sã€GGUF:Q5\_K\_Mã€GGUF:Q6\_Kã€GGUF:Q4\_0ã€GGUF:Q4\_1ã€GGUF:Q5\_0ã€GGUF:Q5\_1ã€GGUF:Q8\_0 |
|**fake**| â€‹`æ‰€æœ‰æ–¹æ¡ˆï¼ˆä»…ç”¨äºç ”ç©¶ï¼‰`                                                                                                                                                                                  |
</details>

### è‡ªé€‚åº”é‡åŒ–æ–¹æ¡ˆï¼ˆAutoSchemeï¼‰ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰

AutoScheme æä¾›äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆç®—æ³•ï¼Œç”¨äºç”Ÿæˆ **è‡ªé€‚åº”çš„æ··åˆç²¾åº¦/æ•°æ®ç±»å‹** çš„é‡åŒ–æ–¹æ¡ˆï¼ˆmixed bits/data type quantization recipesï¼‰ã€‚å…³äº AutoScheme çš„æ›´å¤šç»†èŠ‚å¯å‚è€ƒ[ç”¨æˆ·æŒ‡å—](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme)ã€‚

```python
from auto_round import AutoRound, AutoScheme

model_name = "Qwen/Qwen3-8B"
avg_bits = 3.0
scheme = AutoScheme(avg_bits=avg_bits, options=("GGUF:Q2_K_S", "GGUF:Q4_K_S"), ignore_scale_zp_bits=True)
layer_config = {"lm_head": "GGUF:Q6_K"}

# å¯¹äºé GGUF æ–¹æ¡ˆï¼Œå°† iters æ”¹ä¸º 200
ar = AutoRound(model=model_name, scheme=scheme, layer_config=layer_config, iters=0)
ar.quantize_and_save()
```

<details>
<summary>AutoScheme æ ¸å¿ƒè¶…å‚æ•°è¯´æ˜</summary>

##### AutoScheme è¶…å‚æ•°

- â€‹**â€‹`avg_bits`â€‹**â€‹  **(float)** ï¼šæ•´ä¸ªæ¨¡å‹çš„ç›®æ ‡å¹³å‡ bitsï¼ˆå¹³å‡ bits çš„è®¡ç®—ä»…åŒ…å«è¢«é‡åŒ–çš„å±‚ï¼‰ã€‚
- â€‹**â€‹`options`â€‹**â€‹  **(str | list[str] | list[QuantizationScheme])** â€‹ï¼šé€‰å€™é‡åŒ–æ–¹æ¡ˆé›†åˆã€‚æ”¯æŒä»¥ä¸‹è¡¨ç¤ºå½¢å¼ï¼šå•ä¸ªç”¨é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ `"W4A16,W2A16"`â€‹ï¼‰ã€å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä¾‹å¦‚ `["W4A16", "W2A16"]`â€‹ï¼‰å’Œ `QuantizationScheme` ã€‚
- â€‹**â€‹`ignore_scale_zp_bits`â€‹**â€‹  **(bool)** â€‹ï¼šä»…æ”¯æŒ API è°ƒç”¨åœºæ™¯ã€‚ç”¨äºå†³å®šåœ¨è®¡ç®—å¹³å‡ bit æ—¶ï¼Œæ˜¯å¦å¿½ç•¥ scale ä¸ zero-point çš„ä½æ•°ï¼ˆé»˜è®¤ `False`ï¼‰ã€‚
- â€‹**â€‹`shared_layers`â€‹**â€‹  **(Iterable[Iterable[str]], optional)** ï¼šä»…æ”¯æŒ API è°ƒç”¨åœºæ™¯ï¼Œç”¨äºå®šä¹‰å¤šä¸ªå±‚çš„åˆ†ç»„ï¼Œè¿™äº›å±‚å°†å…±äº«ç›¸åŒçš„é‡åŒ–é…ç½®ã€‚
- â€‹**â€‹`batch_size`â€‹**â€‹  **(int, optional)** â€‹ï¼šä»…æ”¯æŒ API è°ƒç”¨åœºæ™¯ã€‚è®¾ä¸º `1` å¯ä»¥é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†åŒæ—¶ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ã€‚

</details>

### è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ API è°ƒç”¨æ–¹æ³•

å¦‚æœåœ¨é‡åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯å°è¯•å°† `iters` è®¾ç½®ä¸º `0`ï¼ˆRTN æ¨¡å¼ï¼‰ã€å°† `group_size` è®¾ä¸º `32` å¹¶ä¸”æ‰“å¼€ `disable_opt_rtn` ï¼Œé€šå¸¸ä¼šæœ‰å¸®åŠ©ã€‚


<details>
  <summary>ç‚¹å‡»å±•å¼€</summary>

**è¯¥åŠŸèƒ½ä»åœ¨å®éªŒé˜¶æ®µï¼Œå› æ­¤åç»­ç‰ˆæœ¬ä¸­å¯èƒ½æœ‰å˜åŠ¨ã€‚**

é»˜è®¤æƒ…å†µä¸‹ï¼ŒAutoRound åªä¼šé‡åŒ– VLM çš„æ–‡æœ¬æ¨¡å—ï¼Œå¹¶é»˜è®¤é‡‡ç”¨ `NeelNanda/pile-10k`â€‹ ä½œä¸ºæ ‡å®šæ•°æ®é›†ã€‚è‹¥éœ€é‡åŒ–æ•´ä¸ªæ¨¡å‹ï¼Œå¯è®¾ç½® `quant_nontext_module = True` ï¼ˆä½†ç›®å‰ä¸ºæ­¢è¯¥åŠŸèƒ½çš„é€‚ç”¨èŒƒå›´ä»è¾ƒä¸ºæœ‰é™ï¼‰ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [readme](./auto_round/mllm/README.md)

```python
from auto_round import AutoRound

# åŠ è½½æ¨¡å‹
model_name_or_path = "Qwen/Qwen2.5-VL-7B-Instruct"
# é‡åŒ–æ¨¡å‹
ar = AutoRound(model_name_or_path, scheme="W4A16")
output_dir = "./qmodel"
ar.quantize_and_save(output_dir)
```

</details>



## æ¨¡å‹æ¨ç†

### vLLMï¼ˆCPU / Intel GPU / CUDAï¼‰

```python
from vllm import LLM, SamplingParams

prompts = [
    "Hello, my name is",
]
sampling_params = SamplingParams(temperature=0.6, top_p=0.95)
model_name = "Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound"
llm = LLM(model=model_name)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

### SGLangï¼ˆIntel GPU / CUDAï¼‰

**æ³¨æ„ï¼šç›®å‰å¯¹æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€ï¼ˆVLMï¼‰æ¨¡å‹çš„æ”¯æŒå°šä¸å®Œå–„ã€‚**

```python
import sglang as sgl

llm = sgl.Engine(model_path="Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound")
prompts = [
    "Hello, my name is",
]
sampling_params = {"temperature": 0.6, "top_p": 0.95}

outputs = llm.generate(prompts, sampling_params)
for prompt, output in zip(prompts, outputs):
    print(f"Prompt: {prompt}\nGenerated text: {output['text']}")
```

### Transformersï¼ˆCPU / Intel GPU / Gaudi / CUDAï¼‰

AutoRound æ”¯æŒåä½™ç§æ¨ç†åç«¯ï¼Œå¹¶ä¼šæ ¹æ®å·²å®‰è£…çš„åº“è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯ï¼›å¦‚æœæ£€æµ‹åˆ°ç³»ç»Ÿä¸­å­˜åœ¨æ›´ä¼˜åç«¯ä½†ç¼ºå°‘ç›¸å…³ä¾èµ–ï¼Œä¹Ÿä¼šä¸»åŠ¨æç¤ºç”¨æˆ·å®‰è£…ã€‚

â€‹**è¯·å‹¿åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ‰‹åŠ¨å°†é‡åŒ–åçš„æ¨¡å‹è¿ç§»åˆ°å…¶ä»–è®¾å¤‡**â€‹ï¼ˆä¾‹å¦‚æ‰§è¡Œ `model.to('cpu')`ï¼‰ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ„å¤–é”™è¯¯ã€‚

ç›®å‰å¯¹ Gaudi è®¾å¤‡çš„æ”¯æŒå°šä¸å®Œå–„ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "There is a girl who likes adventure,"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50)[0]))
```

## ç ”ç©¶æˆæœ & å…¶ä»–æ´»åŠ¨

[SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs](https://arxiv.org/abs/2512.04746)ï¼ˆ202512 è®ºæ–‡ï¼‰

[Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLM](https://aclanthology.org/2024.findings-emnlp.662/)ï¼ˆ202309 è®ºæ–‡ï¼‰

[TEQ: Trainable Equivalent Transformation for Quantization of LLMs](https://arxiv.org/abs/2310.10944)ï¼ˆ202310 è®ºæ–‡ï¼‰

[Effective Post-Training Quantization for Large Language Models](https://medium.com/intel-analytics-software/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98)ï¼ˆ202304 åšå®¢ï¼‰

æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹ [å®Œæ•´è®ºæ–‡åˆ—è¡¨](./docs/publication_list.md).

## è‡´è°¢

ç‰¹åˆ«æ„Ÿè°¢ AutoGPTQã€AutoAWQã€GPTQModelã€Tritonã€Marlinã€ExLLaMAV2 ç­‰å¼€æºä½ç²¾åº¦åº“ï¼Œå®ƒä»¬æä¾›çš„ä½ç²¾åº¦ CUDA å†…æ ¸ï¼ˆlow-precision CUDA kernelï¼‰ä¸º AutoRound çš„å®ç°æä¾›äº†é‡è¦çš„æ”¯æŒã€‚

## ğŸŒŸ æ”¯æŒæˆ‘ä»¬

å¦‚æœè§‰å¾— AutoRound å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç»™ repo ç‚¹ä¸ª â­ å¹¶è½¬å‘åˆ°ä½ çš„ç¤¾åŒº~

