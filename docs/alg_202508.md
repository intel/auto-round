We use **lm-eval** for evaluation. For LLaMA, we enabled `add_bos_token` and
`removed @use_kernel_forward_from_hub("RMSNorm")`
in [modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L52C1-L52C40)
to stabilize accuracy during evaluation. All other settings follow the default configurations of AutoRound and lm-eval.

| Qwen3-8B W2G64    | Avg.   | arc_challenge | hellaswag | gsm8k  | lambada_openai | mmlu   | mmlupro | truthfulqa_mc1 | winogrande |
|-------------------|--------|---------------|-----------|--------|----------------|--------|---------|----------------|------------|
| AutoRound         | 0.4373 | 0.4019        | 0.4437    | 0.4215 | 0.4826         | 0.5474 | 0.263   | 0.3072         | 0.6314     |
| AutoRound+alg_ext | 0.4787 | 0.4275        | 0.4516    | 0.5944 | 0.5181         | 0.5773 | 0.2807  | 0.3305         | 0.6496     |

| Llama3.1-8B W2G64 | Avg.   | arc_challenge | hellaswag | gsm8k  | lambada_openai | mmlu   | mmlupro | truthfulqa_mc1 | winogrande |
|-------------------|--------|---------------|-----------|--------|----------------|--------|---------|----------------|------------|
| AutoRound         | 0.382  | 0.3635        | 0.4562    | 0.1622 | 0.5069         | 0.4411 | 0.1661  | 0.3207         | 0.6393     |
| AutoRound+alg_ext | 0.4166 | 0.3712        | 0.4729    | 0.2039 | 0.5946         | 0.4981 | 0.2163  | 0.3011         | 0.6748     |