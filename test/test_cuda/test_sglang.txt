import json
import os
import shutil
import sys
import unittest

sys.path.insert(0, "../..")
from pathlib import Path
import sglang as sgl
import torch
from transformers import AutoModelForCausalLM, AutoRoundConfig, AutoTokenizer
from auto_round import AutoRound

class LLMDataLoader:
    def __init__(self):
        self.batch_size = 1

    def __iter__(self):
        for i in range(2):
            yield torch.ones([1, 10], dtype=torch.long)


class TestAutoRoundSGL(unittest.TestCase):
    @classmethod
    def setUpClass(self):
        self.model_name = "/models/opt-125m"
        self.save_dir = "./saved"
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype="auto", trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        self.llm_dataloader = LLMDataLoader()

    @classmethod
    def tearDownClass(self):
        shutil.rmtree("./saved", ignore_errors=True)
        shutil.rmtree("runs", ignore_errors=True)

    def test_ar_format_sglang(self):
        autoround = AutoRound(
            self.model_name,
            scheme="W4A16",
            iters=2,
            seqlen=2,
            dataset=self.llm_dataloader,
        )
        quantized_model_path = self.save_dir
        compressed, _ = autoround.quantize_and_save(
            output_dir=quantized_model_path, inplace=False, format="auto_round"
        )
        import sglang as sgl
        llm = sgl.Engine(model_path=quantized_model_path)
        prompts = [
            "Hello, my name is",
        ]
        sampling_params = {"temperature": 0.6, "top_p": 0.95}
        outputs = llm.generate(prompts, sampling_params)
        generated_text = outputs[0]['text']
        print(f"{generated_text}")
        assert "!!!" not in generated_text
        shutil.rmtree(quantized_model_path, ignore_errors=True)


    def test_mixed_ar_format_sglang(self):
        layer_config = {
            "self_attn": {"bits": 16, "act_bits": 16},
            "lm_head": {"bits": 16, "act_bits": 16},
            "fc1": {"bits": 16, "act_bits": 16},
        }
        autoround = AutoRound(
            self.model_name,
            scheme="W4A16",
            iters=2,
            seqlen=2,
            dataset=self.llm_dataloader,
            layer_config=layer_config,
        )
        quantized_model_path = self.save_dir
        compressed, _ = autoround.quantize_and_save(
            output_dir=quantized_model_path, inplace=False, format="auto_round"
        )
        llm = sgl.Engine(model_path=quantized_model_path)
        prompts = [
            "Hello, my name is",
        ]
        sampling_params = {"temperature": 0.6, "top_p": 0.95}
        outputs = llm.generate(prompts, sampling_params)
        generated_text = outputs[0]['text']
        print(f"{generated_text}")
        assert "!!!" not in generated_text
        shutil.rmtree(quantized_model_path, ignore_errors=True)


if __name__ == "__main__":
    unittest.main()

